{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import pathlib\n",
    "from typing import Callable, ClassVar, Type\n",
    "\n",
    "import pytorch_lightning as lightning\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn\n",
    "\n",
    "# The src.vak.models prefix has to be removed in the actual implementation\n",
    "from src.vak.models.registry import model_family\n",
    "from src.vak.models import base\n",
    "from src.vak.models.definition import ModelDefinition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vak.nn.loss.vae\n",
    "def vae_loss(\n",
    "    x: torch.Tensor,\n",
    "    z: torch.Tensor,\n",
    "    x_rec: torch.Tensor,\n",
    "    latent_dist: torch.Tensor,\n",
    "    model_precision: float,\n",
    "    z_dim: int\n",
    "):\n",
    "\n",
    "    x_dim = x.shape\n",
    "    elbo = -0.5 * ( torch.sum( torch.pow(z, 2) ) + z_dim * np.log( 2 * np.pi ))\n",
    "    # E_{q(z|x)} p(x|z)\n",
    "    pxz_term = -0.5 * x_dim * (np.log(2 * np.pi / model_precision))\n",
    "    l2s = torch.sum( torch.pow( x.view( x.shape[0], -1 ) - x_rec, 2), dim=1)\n",
    "    pxz_term = pxz_term - 0.5 * model_precision * torch.sum(l2s)\n",
    "    elbo = elbo + pxz_term\n",
    "    # H[q(z|x)]\n",
    "    elbo = elbo + torch.sum(latent_dist.entropy())\n",
    "    return elbo\n",
    "\n",
    "class VaeLoss(torch.nn.Module):\n",
    "    \"\"\"\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        return_latent_rec: bool = False,\n",
    "        model_precision: float = 10.0,\n",
    "        z_dim: int = 32\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.return_latent_rec = return_latent_rec\n",
    "        self.model_precision = model_precision\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        z: torch.Tensor,\n",
    "        x_rec: torch.Tensor,\n",
    "        latent_dist: torch.Tensor,\n",
    "    ):\n",
    "        x_shape = x.shape\n",
    "        elbo = vae_loss(x=x, z=z, x_rec=x_rec, latent_dist=latent_dist, model_precision=self.model_precision, z_dim=self.z_dim)\n",
    "        if self.return_latent_rec:\n",
    "            return -elbo, z.detach().cpu().numpy(), \\\n",
    "                x_rec.view(-1, x_shape[0], x_shape[1]).detach().cpu().numpy()\n",
    "        return -elbo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mb/Library/Mobile Documents/com~apple~CloudDocs/gits/vak/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# vak.models.vae_model.VAEModel\n",
    "@model_family\n",
    "class VAEModel(base.Model):\n",
    "    definition: ClassVar[ModelDefinition]\n",
    "    def __init__(\n",
    "        self,\n",
    "        network: dict | None = None,\n",
    "        loss: torch.nn.Module | Callable | None = None,\n",
    "        optimizer: torch.optim.Optimizer | None = None,\n",
    "        metrics: dict[str:Type] | None = None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            network=network, loss=loss, optimizer=optimizer, metrics=metrics\n",
    "        )\n",
    "        self.encoder = network['encode']\n",
    "        self.decoder = network['decode']\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "    \n",
    "    def decode(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self.optimizer\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(\n",
    "        cls, config: dict\n",
    "    ):\n",
    "        network, loss, optimizer, metrics = cls.attributes_from_config(config)\n",
    "        return cls(\n",
    "            network=network,\n",
    "            optimizer=optimizer,\n",
    "            loss=loss,\n",
    "            metrics=metrics,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nets.Ava\n",
    "class Ava(nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dims: List[int] = [8, 8, 16, 16, 24, 24, 32]\n",
    "\t\tfc_dims: List[int] = [1024, 256, 64, 32]\n",
    "\t\tin_channels: int = 1,\n",
    "\t\tin_fc: int = 8192,\n",
    "\t\tx_shape = tuple = (128, 128)\n",
    "\t\t\n",
    "    ):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\t\tself.in_fc = in_fc\n",
    "\t\tself.in_channels = in_channels\n",
    "\t\tself.x_shape = x_shape \n",
    "\t\tself.x_dim = np.prod(x_shape)\n",
    "\t\tmodules = []\n",
    "\t\tfor h_dim in hidden_dims:\n",
    "\t\t\tstride = 2 if h_dim == in_channels else 1\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "\t\t\t\t\tnn.BatchNorm2d(in_channels),\n",
    "                    nn.Conv2d(in_channels, out_channels=h_dim,\n",
    "                              kernel_size=3, stride=stride, padding=1),\n",
    "                    nn.ReLU())\n",
    "            )\n",
    "            in_channels = h_dim\n",
    "\t\t\n",
    "\t\tself.encoder = nn.Sequential(*modules)\n",
    "\t\t\n",
    "\t\tmodules = []\n",
    "\t\tfor fc_dim in fc_dims[:-2]:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "\t\t\t\t\tnn.Linear(in_fc, fc_dim),\n",
    "                    nn.ReLU())\n",
    "            )\n",
    "            in_fc = fc_dim\n",
    "\t\tself.encoder_bottleneck = nn.Sequential(*modules)\n",
    "\n",
    "\t\tself.mu_layer = nn.Sequential(\n",
    "\t\t\tnn.Linear(fc_dims[-3], fc_dims[-2]),\n",
    "            nn.ReLU(),\n",
    "\t\t\tnn.Linear(fc_dims[-2], fc_dims[-1]))\n",
    "\t\t\n",
    "\t\tself.u_layer = nn.Sequential(\n",
    "\t\t\tnn.Linear(fc_dims[-3], fc_dims[-2]),\n",
    "            nn.ReLU(),\n",
    "\t\t\tnn.Linear(fc_dims[-2], fc_dims[-1]))\n",
    "\t\t\n",
    "\t\tself.d_layer = nn.Sequential(\n",
    "\t\t\tnn.Linear(fc_dims[-3], fc_dims[-2]),\n",
    "            nn.ReLU(),\n",
    "\t\t\tnn.Linear(fc_dims[-2], fc_dims[-1]))\n",
    "\n",
    "\t\tfc_dims.reverse()\n",
    "\t\tmodules = []\n",
    "\t\tfor i in range(len(fc_dims)):\n",
    "\t\t\tout = self.fc_in if i == len(fc_dims) else fc_dims[i+1]\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "\t\t\t\t\tnn.Linear(fc_dims[i], out),\n",
    "                    nn.ReLU())\n",
    "            )\n",
    "\t\tself.decoder_bottleneck = nn.Sequential(*modules)\n",
    "        \n",
    "\t\thidden_dims.reverse()\n",
    "\t\tmodules = []\n",
    "\t\tfor i, h_dim in enumerate(hidden_dims):\n",
    "\t\t\tstride = 2 if h_dim == in_channels else 1\n",
    "\t\t\toutput_padding = 1 if h_dim == in_channels else 0\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "\t\t\t\t\tnn.BatchNorm2d(in_channels),\n",
    "                    nn.ConvTranspose2d(in_channels, out_channels=h_dim,\n",
    "                              kernel_size=3, stride=stride, padding=1, output_padding=output_padding),\n",
    "                    nn.ReLU() if i != len(hidden_dims))\n",
    "            )\n",
    "            in_channels = h_dim\n",
    "\n",
    "\t\tself.decoder = nn.Sequential(*modules)\n",
    "\n",
    "\tdef encode(self, x):\n",
    "\t\t\"\"\"\n",
    "\t\t\"\"\"\n",
    "\t\tx = self.encoder(x.unsqueeze(self.in_channels)).view(-1, self.in_fc)\n",
    "\t\tx = self.encoder_bottleneck(x)\n",
    "\t\tmu = self.mu_layer(x)\n",
    "\t\tu = self.u_layer(x).unsqueeze(-1)\n",
    "\t\td = torch.exp(self.d_layer(x))\n",
    "\t\treturn mu, u, d\n",
    "\n",
    "\n",
    "\tdef decode(self, z):\n",
    "\t\t\"\"\"\n",
    "\t\t\"\"\"\n",
    "\t\tz = self.decoder_bottleneck(z).view(-1,32,16,16)\n",
    "\t\tz = self.decoder(z).view(-1, x_dim)\n",
    "\t\treturn z\n",
    "\n",
    "    def reparametrize(self, mu, u, d):\n",
    "        latent_dist = LowRankMultivariateNormal(mu, u, d)\n",
    "\t\tz = latent_dist.rsample()\n",
    "        return z, latent_dist\n",
    "\n",
    "\n",
    "\tdef forward(self, x, return_latent_rec=False):\n",
    "\t\tmu, u, d = self.encode(x)\n",
    "\t\tz, latent_dist = self.reparametrize(mu, u, d)\n",
    "\t\tx_rec = self.decode(z)\n",
    "\t\treturn x_rec, {'z': z, 'mu': mu, 'latent_dist': latent_dist, 'u': u, 'd': d }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model(family=VAEModel)\n",
    "class AvaNet: # this will be renamed to Ava in implementation, just to avoid naming conflicts.\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    network = Ava\n",
    "    loss = VaeLoss\n",
    "    optimizer = torch.optim.Adam\n",
    "    metrics = {\n",
    "        \"loss\": VaeLoss,\n",
    "    }\n",
    "    default_config = {\"optimizer\": {\"lr\": 0.003}}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
