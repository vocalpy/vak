from __future__ import annotations

import pathlib
from typing import Callable

import crowsetta
import numpy.typing as npt
import pandas as pd

from ..common import annotation, files
from .. import transforms


class VocalDataset:
    """Base class to represent a dataset of vocalizations.
    It assumes that the dataset consists of spectrograms of
    vocalizations and, optionally, annotations
    for those vocalizations.
    """

    def __init__(
        self,
        root: str | pathlib.Path,
        spect_paths: npt.NDArray,
        annots: list[crowsetta.Annotation] | None = None,
        labelmap: dict = None,
        spect_key: str = "s",
        timebins_key: str = "t",
        item_transform: Callable | None = None,
    ):
        """initialize a VocalDataset instance

        Parameters
        ----------
        root : str, pathlib.Path
            Path to directory that contains dataset, generated by ``vak prep``.
            Name 'root' is used for consistency with torchvision.datasets.
        spect_paths : numpy.ndarray
            column from DataFrame that represents dataset,
            consisting of paths to files containing spectrograms as arrays
        annots : list
            of crowsetta.Annotation instances,
            loaded from from DataFrame that represents dataset, using vak.annotation.from_df.
            Default is None, in which case no annotation is returned with each item
            in the dataset.
        labelmap : dict
            that maps labels from dataset to a series of consecutive integer.
            To create a label map, pass a set of labels to the `vak.utils.labels.to_map` function.
        spect_key : str
            key to access spectograms in array files. Default is 's'.
        timebins_key : str
            key to access time bin vector in array files. Default is 't'.
        item_transform : callable
            A function / transform that takes an input numpy array or torch Tensor,
            and optionally a target array or Tensor, and returns a dictionary.
            This dictionary is the item returned when indexing into the dataset.
            Default is None.
        """
        self.root = pathlib.Path(root)
        if not root.exists() or not root.is_dir():
            raise NotADirectoryError(
                f"`root` not found or not recognized as a directory: {root}"
            )

        self.spect_paths = spect_paths
        self.spect_key = spect_key
        self.timebins_key = timebins_key
        self.annots = annots
        self.labelmap = labelmap
        if "unlabeled" in self.labelmap:
            self.unlabeled_label = self.labelmap["unlabeled"]
        else:
            # if there is no "unlabeled label" (e.g., because all segments have labels)
            # just assign dummy value that will end up getting replaced by actual labels by label_timebins()
            self.unlabeled_label = 0
        self.item_transform = item_transform

        tmp_x_ind = 0
        tmp_item = self.__getitem__(tmp_x_ind)
        # used by vak functions that need to determine size of input,
        # e.g. when initializing a neural network model
        self.shape = tmp_item["source"].shape

    def __getitem__(self, idx):
        spect_path = self.spect_paths[idx]
        spect_dict = files.spect.load(self.root / spect_path)
        spect = spect_dict[self.spect_key]

        if self.annots is not None:
            timebins = spect_dict[self.timebins_key]

            annot = self.annots[idx]
            lbls_int = [self.labelmap[lbl] for lbl in annot.seq.labels]
            # "lbl_tb": labeled timebins. Target for output of network
            lbl_tb = transforms.labeled_timebins.from_segments(
                lbls_int,
                annot.seq.onsets_s,
                annot.seq.offsets_s,
                timebins,
                unlabeled_label=self.unlabeled_label,
            )
            item = self.item_transform(spect, lbl_tb, spect_path)
        else:
            item = self.item_transform(spect, spect_path)

        return item

    def __len__(self):
        """number of batches"""
        return len(self.spect_paths)

    @classmethod
    def from_csv(
        cls,
        dataset_csv_path: str | pathlib.Path,
        split: str,
        labelmap: dict,
        spect_key: str = "s",
        timebins_key: str = "t",
        item_transform: Callable | None = None,
    ):
        """given a path to a csv representing a dataset,
        returns an initialized VocalDataset.

        Parameters
        ----------
        dataset_csv_path : str, Path
            path to a .csv file that represents the dataset.
        spect_paths : numpy.ndarray
            column from DataFrame that represents dataset,
            consisting of paths to files containing spectrograms as arrays
        annots : list
            of crowsetta.Annotation instances,
            loaded from DataFrame that represents dataset, using vak.annotation.from_df.
            Default is None, in which case no annotation is returned with each item
            in the dataset.
        labelmap : dict
            that maps labels from dataset to a series of consecutive integer.
            To create a label map, pass a set of labels to the `vak.utils.labels.to_map` function.
        spect_key : str
            key to access spectograms in array files. Default is 's'.
        timebins_key : str
            key to access time bin vector in array files. Default is 't'.
        item_transform : callable
            A function / transform that takes an input numpy array or torch Tensor,
            and optionally a target array or Tensor, and returns a dictionary.
            This dictionary is the item returned when indexing into the dataset.
            Default is None.

        Returns
        -------
        initialized instance of VocalDataset
        """
        dataset_csv_path = pathlib.Path(dataset_csv_path)
        if not dataset_csv_path.exists():
            raise FileNotFoundError(
                f"`dataset_csv_path` not found or not recognized as a directory: {dataset_csv_path}"
            )
        dataset_path = dataset_csv_path.parent

        df = pd.read_csv(dataset_csv_path)
        if not df["split"].str.contains(split).any():
            raise ValueError(f"split {split} not found in dataset in csv: {dataset_csv_path}")
        else:
            df = df[df["split"] == split]

        spect_paths = df["spect_path"].values

        # below, annots will be None if no format is specified in the `annot_format` column of the dataframe.
        # this is intended behavior; makes it possible to use same dataset class for prediction
        annots = annotation.from_df(df, dataset_path)

        return cls(
            dataset_path,
            spect_paths,
            annots,
            labelmap,
            spect_key,
            timebins_key,
            item_transform,
        )
