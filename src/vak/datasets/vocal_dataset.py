from __future__ import annotations

import pathlib
from typing import Callable

import crowsetta
import numpy.typing as npt
import pandas as pd

from .. import annotation
from .. import files
from .. import transforms


class VocalDataset:
    """Base class to represent a dataset of vocalizations.
    It assumes that the dataset consists of spectrograms of
    vocalizations and, optionally, annotations
    for those vocalizations.
    """

    def __init__(
        self,
        root: str | pathlib.Path,
        spect_paths: npt.NDArray,
        annots: list[crowsetta.Annotation] | None = None,
        labelmap: dict = None,
        spect_key: str = "s",
        timebins_key: str = "t",
        item_transform: Callable | None = None,
    ):
        """initialize a VocalDataset instance

        Parameters
        ----------
        root : str, pathlib.Path
            Path to directory that contains dataset, generated by ``vak prep``.
            Name 'root' is used for consistency with torchvision.datasets.
        spect_paths : numpy.ndarray
            column from DataFrame that represents dataset,
            consisting of paths to files containing spectrograms as arrays
        annots : list
            of crowsetta.Annotation instances,
            loaded from from DataFrame that represents dataset, using vak.annotation.from_df.
            Default is None, in which case no annotation is returned with each item
            in the dataset.
        labelmap : dict
            that maps labels from dataset to a series of consecutive integer.
            To create a label map, pass a set of labels to the `vak.utils.labels.to_map` function.
        spect_key : str
            key to access spectograms in array files. Default is 's'.
        timebins_key : str
            key to access time bin vector in array files. Default is 't'.
        item_transform : callable
            A function / transform that takes an input numpy array or torch Tensor,
            and optionally a target array or Tensor, and returns a dictionary.
            This dictionary is the item returned when indexing into the dataset.
            Default is None.
        """
        self.root = pathlib.Path(root)
        if not root.exists() or not root.is_dir():
            raise NotADirectoryError(
                f"`root` not found or not recognized as a directory: {root}"
            )

        self.spect_paths = spect_paths
        self.spect_key = spect_key
        self.timebins_key = timebins_key
        self.annots = annots
        self.labelmap = labelmap
        if "unlabeled" in self.labelmap:
            self.unlabeled_label = self.labelmap["unlabeled"]
        else:
            # if there is no "unlabeled label" (e.g., because all segments have labels)
            # just assign dummy value that will end up getting replaced by actual labels by label_timebins()
            self.unlabeled_label = 0
        self.item_transform = item_transform

        tmp_x_ind = 0
        tmp_item = self.__getitem__(tmp_x_ind)
        # used by vak functions that need to determine size of input,
        # e.g. when initializing a neural network model
        self.shape = tmp_item["source"].shape

    def __getitem__(self, idx):
        spect_path = self.spect_paths[idx]
        spect_dict = files.spect.load(self.root / spect_path)
        spect = spect_dict[self.spect_key]

        if self.annots is not None:
            timebins = spect_dict[self.timebins_key]

            annot = self.annots[idx]
            lbls_int = [self.labelmap[lbl] for lbl in annot.seq.labels]
            # "lbl_tb": labeled timebins. Target for output of network
            lbl_tb = transforms.labeled_timebins.from_segments(
                lbls_int,
                annot.seq.onsets_s,
                annot.seq.offsets_s,
                timebins,
                unlabeled_label=self.unlabeled_label,
            )
            item = self.item_transform(spect, lbl_tb, spect_path)
        else:
            item = self.item_transform(spect, spect_path)

        return item

    def __len__(self):
        """number of batches"""
        return len(self.spect_paths)

    @classmethod
    def from_csv(
        cls,
        dataset_csv_path: str | pathlib.Path,
        split: str,
        labelmap: dict,
        spect_key: str = "s",
        timebins_key: str = "t",
        item_transform: Callable | None = None,
    ):
        """given a path to a csv representing a dataset,
        returns an initialized VocalDataset.

        Parameters
        ----------
        dataset_csv_path : str, Path
            path to a .csv file that represents the dataset.
        spect_paths : numpy.ndarray
            column from DataFrame that represents dataset,
            consisting of paths to files containing spectrograms as arrays
        annots : list
            of crowsetta.Annotation instances,
            loaded from DataFrame that represents dataset, using vak.annotation.from_df.
            Default is None, in which case no annotation is returned with each item
            in the dataset.
        labelmap : dict
            that maps labels from dataset to a series of consecutive integer.
            To create a label map, pass a set of labels to the `vak.utils.labels.to_map` function.
        spect_key : str
            key to access spectograms in array files. Default is 's'.
        timebins_key : str
            key to access time bin vector in array files. Default is 't'.
        item_transform : callable
            A function / transform that takes an input numpy array or torch Tensor,
            and optionally a target array or Tensor, and returns a dictionary.
            This dictionary is the item returned when indexing into the dataset.
            Default is None.

        Returns
        -------
        initialized instance of VocalDataset
        """
        dataset_csv_path = pathlib.Path(dataset_csv_path)
        if not dataset_csv_path.exists():
            raise FileNotFoundError(
                f"`dataset_csv_path` not found or not recognized as a directory: {dataset_csv_path}"
            )
        dataset_path = dataset_csv_path.parent

        df = pd.read_csv(dataset_csv_path)
        if not df["split"].str.contains(split).any():
            raise ValueError(f"split {split} not found in dataset in csv: {dataset_csv_path}")
        else:
            df = df[df["split"] == split]

        spect_paths = df["spect_path"].values

        # below, annots will be None if no format is specified in the `annot_format` column of the dataframe.
        # this is intended behavior; makes it possible to use same dataset class for prediction
        annots = annotation.from_df(df, dataset_path)

        return cls(
            dataset_path,
            spect_paths,
            annots,
            labelmap,
            spect_key,
            timebins_key,
            item_transform,
        )
