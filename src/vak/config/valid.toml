# This .toml file has all valid options for each section.
# The values for those options are not used, they are just dummy values.
# The file is used by parse.py to determine whether a .toml file
# provided by the user has valid sections + options.
# Options should be in the same order they are defined for the
# attrs-based class that represents the config, for easy comparison
# when changing that class + this file.
[PREP]
data_dir = './tests/test_data/cbins/gy6or6/032312'
output_dir = './tests/test_data/prep/learncurve'
dataset_type = 'frame_classification'
input_type = 'spect'
audio_format = 'cbin'
spect_format = 'npz'
annot_format = 'notmat'
annot_file = './some/annot/file'
labelset = 'iabcdefghjk'
audio_dask_bag_kwargs = { npartitions = 20}
train_dur = 50
val_dur = 15
test_dur = 30
train_set_durs = [ 4.5, 6.0 ]
num_replicates = 2

[SPECT_PARAMS]
fft_size = 512
step_size = 64
freq_cutoffs = [ 500, 10000 ]
thresh = 6.25
transform_type = 'log_spect'
spect_key = 's'
freqbins_key = 'f'
timebins_key = 't'
audio_path_key = 'audio_path'

[TRAIN]
model = 'TweetyNet'
root_results_dir = './tests/test_data/results/train'
dataset_path = 'tests/test_data/prep/train/032312_prep_191224_225912.csv'
num_workers = 4
device = 'cuda'
batch_size = 11
num_epochs = 2
normalize_spectrograms = true
shuffle = true
val_step = 1
ckpt_step = 1
patience = 4
results_dir_made_by_main_script = '/some/path/to/learncurve/'
checkpoint_path = '/home/user/results_181014_194418/TweetyNet/checkpoints/'
spect_scaler_path = '/home/user/results_181014_194418/spect_scaler'
train_transform_params = {'resize' = 128}
train_dataset_params = {'window_size' = 80}
val_transform_params = {'resize' = 128}
val_dataset_params = {'window_size' = 80}

[EVAL]
dataset_path = 'tests/test_data/prep/learncurve/032312_prep_191224_225910.csv'
checkpoint_path = '/home/user/results_181014_194418/TweetyNet/checkpoints/'
labelmap_path = '/home/user/results_181014_194418/labelmap.json'
output_dir = './tests/test_data/prep/learncurve'
model = 'TweetyNet'
batch_size = 11
num_workers = 4
device = 'cuda'
spect_scaler_path = '/home/user/results_181014_194418/spect_scaler'
post_tfm_kwargs = {'majority_vote' = true, 'min_segment_dur' = 0.01}
transform_params = {'resize' = 128}
dataset_params = {'window_size' = 80}

[LEARNCURVE]
model = 'TweetyNet'
root_results_dir = './tests/test_data/results/learncurve'
batch_size = 11
num_epochs = 2
normalize_spectrograms = true
shuffle = true
val_step = 1
ckpt_step = 1
patience = 4
dataset_path = 'tests/test_data/prep/learncurve/032312_prep_191224_225910.csv'
results_dir_made_by_main_script = '/some/path/to/learncurve/'
post_tfm_kwargs = {'majority_vote' = true, 'min_segment_dur' = 0.01}
num_workers = 4
device = 'cuda'
train_transform_params = {'resize' = 128}
train_dataset_params = {'window_size' = 80}
val_transform_params = {'resize' = 128}
val_dataset_params = {'window_size' = 80}

[PREDICT]
dataset_path = 'tests/test_data/prep/learncurve/032312_prep_191224_225910.csv'
checkpoint_path = '/home/user/results_181014_194418/TweetyNet/checkpoints/'
labelmap_path = '/home/user/results_181014_194418/labelmap.json'
annot_csv_filename = '032312_prep_191224_225910.annot.csv'
output_dir = './tests/test_data/prep/learncurve'
model = 'TweetyNet'
batch_size = 11
num_workers = 4
device = 'cuda'
spect_scaler_path = '/home/user/results_181014_194418/spect_scaler'
min_segment_dur = 0.004
majority_vote = false
save_net_outputs = false
transform_params = {'resize' = 128}
dataset_params = {'window_size' = 80}
