# PREP: options for preparing dataset
[vak.prep]
# dataset_type: corresponds to the model family such as "frame classification" or "parametric umap"
dataset_type = "frame classification"
# input_type: input to model, either audio ("audio") or spectrogram ("spect")
input_type = "spect"
# data_dir: directory with data to use when preparing dataset
data_dir = "/PATH/TO/FOLDER/gy6or6/032312"
# output_dir: directory where dataset will be created (as a sub-directory within output_dir)
output_dir = "/PATH/TO/FOLDER/prep/predict"
# audio_format: format of audio, either wav or cbin
audio_format = "wav"
# note that for predictions we don't need to specify labelset or annot_format
# note also that we do not specify train_dur / val_dur / test_dur;
# all data found in `data_dir` will be assigned to a "predict split" instead

# SPECT_PARAMS: parameters for computing spectrograms
[vak.prep.spect_params]
# fft_size: size of window used for Fast Fourier Transform, in number of samples
fft_size = 512
# step_size: size of step to take when computing spectra with FFT for spectrogram
# also known as hop size
step_size = 64

# PREDICT: options for generating predictions with a trained model
[vak.predict]
# checkpoint_path: path to saved model checkpoint
checkpoint_path = "/PATH/TO/FOLDER/results/train/RESULTS_TIMESTAMP/TweetyNet/checkpoints/max-val-acc-checkpoint.pt"
# labelmap_path: path to file that maps from outputs of model (integers) to text labels in annotations;
# this is used when generating predictions
labelmap_path = "/PATH/TO/FOLDER/results/train/RESULTS_TIMESTAMP/labelmap.json"
# frames_standardizer_path: path to file containing SpectScaler that was fit to training set
# We want to transform the data we predict on in the exact same way
frames_standardizer_path = "/PATH/TO/FOLDER/results/train/RESULTS_TIMESTAMP/StandardizeSpect"
# batch_size
# for predictions with a frame classification model, this should always be 1
# and will be ignored if it's not
batch_size = 1
# num_workers: number of workers to use when loading data with multiprocessing
num_workers = 4
# device: name of device to run model on, one of "cuda", "cpu"

# output_dir: directory where output should be saved, as a sub-directory within `output_dir`
output_dir = "/PATH/TO/FOLDER/results/predict"
# annot_csv_filename
annot_csv_filename = "gy6or6.032312.annot.csv"
# The next two options are for post-processing transforms.
# Both these transforms require that there is an "unlabeled" label,
# and they will only be applied to segments that are bordered on both sides
# by the "unlabeled" label.
# Such a label class is added by default by vak.
# majority_vote: post-processing transformation that takes majority vote within segments that
# do not have the 'unlabeled' class label. Only applied if `majority_vote` is `true`
# (default is false).
majority_vote = true
# min_segment_dur: post-processing transformation removes any segments
# with a duration shorter than `min_segment_dur` that do not have the 'unlabeled' class.
# Only applied if this option is specified.
min_segment_dur = 0.01
# dataset_path : path to dataset created by prep. This will be added when you run `vak prep`, you don't have to add it

# dataset.params = parameters used for datasets
# for a frame classification model, we use dataset classes with a specific `window_size`
[vak.predict.dataset]
path = "/copy/path/from/train/config/here"
params = { window_size = 176 }

# We put this table though vak knows which model we are using
[vak.predict.model.TweetyNet.network]
# hidden_size: the number of elements in the hidden state in the recurrent layer of the network
# we trained with hidden size = 256 so we need to evaluate with the same hidden size;
# otherwise we'll get an error about "shapes do not match" when torch tries to load the checkpoint
hidden_size = 256

# this sub-table configures the `lightning.pytorch.Trainer`
[vak.predict.trainer]
# setting to 'gpu' means "train models on 'gpu' (not 'cpu')"
accelerator = "gpu"
# use the first GPU (numbering starts from 0)
devices = [0]
